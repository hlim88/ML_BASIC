{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Flow basic : Lab 7\n",
    "### Some miscell stuffs : from basic\n",
    "Here we discuss several basic miscellaneous stuff to get better performance in our ML\n",
    "#### 1. Learning rate\n",
    "If we choose too big learning rate, it could be *overshooting*. Cost function value will be `nan`. If we choose too small learning rate, it takes too long to reach low cost value or reach local minimum.\n",
    "#### 2. Data (X) preprocessing for gradient descent\n",
    "If data has large eccentricity, we may need some preprocessing. Examples of preprocessing:\n",
    "* zero-centered data : make data center as zero\n",
    "* normalized data : ex) standardization : \n",
    "\n",
    "\\begin{align}\n",
    "x'_j = \\frac{x_j - \\mu_j}{\\sigma_j}\n",
    "\\end{align}\n",
    "\n",
    "In python code, `x_std[:,0] = (x[:,0] - x[:,0].mean())/x[:,0].std()`\n",
    "#### 3. Overfitting\n",
    "Our model is very good with training data set (with memorization) i.e. really good for one specific training data but not good at test dataset or in real use : *overfitting*.\n",
    "\n",
    "To solve this problem : 1. more training data, 2. reduce the number of features, 3. regularization\n",
    "##### Regularization\n",
    "Let's not have too big numbers in the weight. For example, in cost/loss function\n",
    "\n",
    "\\begin{align}\n",
    "L = \\frac{1}{N} \\sum_i D(S(WX_i + b),L_i) + \\lambda \\sum W^2\n",
    "\\end{align}\n",
    "\n",
    "where $\\lambda$ is regularization strength. In `tf`, `l2req = 0.001*tf.reduce_sum(tf.square(W))`\n",
    "#### 4. Performance evaluation\n",
    "If we use all our data as training set, machine will memorize the data and show 100% accuracy for performance evaluation within the data set. Better way is spiltting data set into training set and test set.\n",
    "\n",
    "We also split training set into training part and validation part. The purpose of having validation set is testing different learning rate and regularization strength to find the best optimized way\n",
    "#### 5. Online learning\n",
    "Splitting large of data set to train the model. Note that model should not be initialized after one training set is done. The model accumulate the training set. This is also good for if there will be more data in the training set. Good example is MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 cost: 3.06972 W: [[ 0.01725777 -0.59107554  0.13571417]\n",
      " [ 1.52442694 -0.87716925  0.38636878]\n",
      " [-0.0477597   2.43544221 -0.31850761]]\n",
      "step: 20 cost: 2.72076 W: [[-0.00958261 -0.60094869  0.17242773]\n",
      " [ 1.44495106 -0.89593834  0.48461378]\n",
      " [-0.08107084  2.38224101 -0.23199536]]\n",
      "step: 40 cost: 2.40617 W: [[-0.03285555 -0.61368328  0.20843527]\n",
      " [ 1.38356745 -0.93107784  0.58113682]\n",
      " [-0.09719197  2.31290507 -0.14653832]]\n",
      "step: 60 cost: 2.10418 W: [[-0.05641251 -0.62490511  0.24321403]\n",
      " [ 1.31701732 -0.95770228  0.67431134]\n",
      " [-0.11959592  2.25211549 -0.06334475]]\n",
      "step: 80 cost: 1.82633 W: [[-0.07757981 -0.63605219  0.27552837]\n",
      " [ 1.25870228 -0.9844836   0.75940764]\n",
      " [-0.13586731  2.19127631  0.0137658 ]]\n",
      "step: 100 cost: 1.60297 W: [[-0.09569669 -0.64523464  0.30282772]\n",
      " [ 1.21126258 -1.00113726  0.82350099]\n",
      " [-0.14490171  2.14069176  0.07338482]]\n",
      "step: 120 cost: 1.48679 W: [[-0.10871439 -0.65104318  0.32165396]\n",
      " [ 1.18791807 -0.9996689   0.84537721]\n",
      " [-0.13534266  2.10849571  0.09602183]]\n",
      "step: 140 cost: 1.45899 W: [[-0.11924855 -0.65419042  0.33533537]\n",
      " [ 1.17685223 -0.98348439  0.8402586 ]\n",
      " [-0.11657311  2.09126639  0.09448165]]\n",
      "step: 160 cost: 1.44232 W: [[-0.12996514 -0.65707535  0.34893689]\n",
      " [ 1.16400719 -0.96586722  0.83548647]\n",
      " [-0.09977976  2.07558107  0.09337361]]\n",
      "step: 180 cost: 1.42596 W: [[-0.14040303 -0.66002399  0.36232343]\n",
      " [ 1.15198529 -0.94868177  0.83032292]\n",
      " [-0.08254067  2.05957031  0.09214517]]\n",
      "step: 200 cost: 1.40988 W: [[-0.15073711 -0.6629464   0.37557989]\n",
      " [ 1.13982248 -0.93141413  0.82521808]\n",
      " [-0.06573524  2.04375148  0.09115853]]\n",
      "Prediction: [2 2 2]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# First, do some example as training and test data set\n",
    "\n",
    "# This is our data set for training\n",
    "x_data = [[1,2,1],[1,3,2],[1,3,4],[1,5,5],[1,7,5],[1,2,5],[1,6,6],[1,7,7]]\n",
    "y_data = [[0,0,1],[0,0,1],[0,0,1],[0,1,0],[0,1,0],[0,1,0],[1,0,0],[1,0,0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2,1,1],[3,1,2],[3,3,4]]\n",
    "y_test = [[0,0,1],[0,0,1],[0,0,1]]\n",
    "\n",
    "X = tf.placeholder(\"float\",[None,3])\n",
    "Y = tf.placeholder(\"float\",[None,3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis),axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# correct predicstion test model\n",
    "prediction = tf.arg_max(hypothesis,1)\n",
    "is_correct = tf.equal(prediction,tf.arg_max(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        if step % 20 == 0:\n",
    "            cost_val, W_val, _ = sess.run([cost,W,optimizer],feed_dict={X:x_data,Y:y_data})\n",
    "            print(\"step:\",step,\"cost:\",cost_val,\"W:\",W_val)\n",
    "        \n",
    "    #predict\n",
    "    print(\"Prediction:\",sess.run(prediction,feed_dict={X:x_test}))\n",
    "    \n",
    "    #Calculate accuracy\n",
    "    print(\"Accuracy:\",sess.run(accuracy,feed_dict={X:x_test,Y:y_test}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 cost: 3.02678 W: [[ 1.38688445  0.26892543 -0.2401225 ]\n",
      " [ 1.60748291 -3.12318587  2.04111123]\n",
      " [ 2.07670379 -3.20074105  0.57526666]]\n",
      "step: 20 cost: 21.4738 W: [[ 0.27165532  0.83142543  0.31260675]\n",
      " [-2.4943099  -0.49818587  3.51790428]\n",
      " [-2.03460288 -0.38824105  1.87407351]]\n",
      "step: 40 cost: 28.0061 W: [[ 0.64665532  1.3939234  -0.62489128]\n",
      " [-0.0568099   2.12681031 -1.5445919 ]\n",
      " [ 0.40289712  2.4242568  -3.37592459]]\n",
      "step: 60 cost: 16.248 W: [[ 1.02130949  0.45676935 -0.0623914 ]\n",
      " [ 2.3799932  -1.80999255 -0.04459214]\n",
      " [ 2.84004569 -1.32539129 -2.06342483]]\n",
      "step: 80 cost: 23.4484 W: [[-0.10368812  1.01926923  0.50010628]\n",
      " [-1.74500203  0.81500697  1.45540321]\n",
      " [-1.28495193  1.48710823 -0.75092721]]\n",
      "step: 100 cost: 9.84287 W: [[ 0.2712957   0.2241323   0.92025948]\n",
      " [ 0.69246554 -2.79538369  2.62832689]\n",
      " [ 1.15253162 -2.07744455  0.37614226]]\n",
      "step: 120 cost: 17.8255 W: [[ 0.61870313  0.7866323   0.01035208]\n",
      " [ 3.07217813 -0.17038369 -2.37638569]\n",
      " [ 3.45991087  0.73505545 -4.74373722]]\n",
      "step: 140 cost: nan W: [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "step: 160 cost: nan W: [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "step: 180 cost: nan W: [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "step: 200 cost: nan W: [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "Prediction: [0 0 0]\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Second, do some learning rate example\n",
    "\n",
    "# Using exact same code but different large learning rate. Results show overshooting\n",
    "x_data = [[1,2,1],[1,3,2],[1,3,4],[1,5,5],[1,7,5],[1,2,5],[1,6,6],[1,7,7]]\n",
    "y_data = [[0,0,1],[0,0,1],[0,0,1],[0,1,0],[0,1,0],[0,1,0],[1,0,0],[1,0,0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2,1,1],[3,1,2],[3,3,4]]\n",
    "y_test = [[0,0,1],[0,0,1],[0,0,1]]\n",
    "\n",
    "X = tf.placeholder(\"float\",[None,3])\n",
    "Y = tf.placeholder(\"float\",[None,3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis),axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.5).minimize(cost)\n",
    "\n",
    "# correct predicstion test model\n",
    "prediction = tf.arg_max(hypothesis,1)\n",
    "is_correct = tf.equal(prediction,tf.arg_max(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        if step % 20 == 0:\n",
    "            cost_val, W_val, _ = sess.run([cost,W,optimizer],feed_dict={X:x_data,Y:y_data})\n",
    "            print(\"step:\",step,\"cost:\",cost_val,\"W:\",W_val)\n",
    "        \n",
    "    #predict\n",
    "    print(\"Prediction:\",sess.run(prediction,feed_dict={X:x_test}))\n",
    "    \n",
    "    #Calculate accuracy\n",
    "    print(\"Accuracy:\",sess.run(accuracy,feed_dict={X:x_test,Y:y_test}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 cost: 10.3224 W: [[-1.21103191  0.19802444 -0.35560009]\n",
      " [-0.02010378 -0.84213227  1.9894762 ]\n",
      " [-0.99732786 -0.42234033  0.30446547]]\n",
      "step: 20 cost: 10.3224 W: [[-1.21103191  0.19802444 -0.35560009]\n",
      " [-0.02010378 -0.84213227  1.9894762 ]\n",
      " [-0.99732786 -0.42234033  0.30446547]]\n",
      "step: 40 cost: 10.3224 W: [[-1.21103191  0.19802444 -0.35560009]\n",
      " [-0.02010378 -0.84213227  1.9894762 ]\n",
      " [-0.99732786 -0.42234033  0.30446547]]\n",
      "step: 60 cost: 10.3224 W: [[-1.21103191  0.19802444 -0.35560009]\n",
      " [-0.02010378 -0.84213227  1.9894762 ]\n",
      " [-0.99732786 -0.42234033  0.30446547]]\n",
      "step: 80 cost: 10.3224 W: [[-1.21103191  0.19802444 -0.35560009]\n",
      " [-0.02010378 -0.84213227  1.9894762 ]\n",
      " [-0.99732786 -0.42234033  0.30446547]]\n",
      "step: 100 cost: 10.3224 W: [[-1.21103191  0.19802444 -0.35560009]\n",
      " [-0.02010378 -0.84213227  1.9894762 ]\n",
      " [-0.99732786 -0.42234033  0.30446547]]\n",
      "step: 120 cost: 10.3224 W: [[-1.21103191  0.19802444 -0.35560009]\n",
      " [-0.02010378 -0.84213227  1.9894762 ]\n",
      " [-0.99732786 -0.42234033  0.30446547]]\n",
      "step: 140 cost: 10.3224 W: [[-1.21103191  0.19802444 -0.35560009]\n",
      " [-0.02010378 -0.84213227  1.9894762 ]\n",
      " [-0.99732786 -0.42234033  0.30446547]]\n",
      "step: 160 cost: 10.3224 W: [[-1.21103191  0.19802444 -0.35560009]\n",
      " [-0.02010378 -0.84213227  1.9894762 ]\n",
      " [-0.99732786 -0.42234033  0.30446547]]\n",
      "step: 180 cost: 10.3224 W: [[-1.21103191  0.19802444 -0.35560009]\n",
      " [-0.02010378 -0.84213227  1.9894762 ]\n",
      " [-0.99732786 -0.42234033  0.30446547]]\n",
      "step: 200 cost: 10.3224 W: [[-1.21103191  0.19802444 -0.35560009]\n",
      " [-0.02010378 -0.84213227  1.9894762 ]\n",
      " [-0.99732786 -0.42234033  0.30446547]]\n",
      "Prediction: [1 1 2]\n",
      "Accuracy: 0.333333\n"
     ]
    }
   ],
   "source": [
    "# Now, using small learning rate\n",
    "# result shows cost function slowly converge or just reach local minimum\n",
    "\n",
    "%reset -f\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1,2,1],[1,3,2],[1,3,4],[1,5,5],[1,7,5],[1,2,5],[1,6,6],[1,7,7]]\n",
    "y_data = [[0,0,1],[0,0,1],[0,0,1],[0,1,0],[0,1,0],[0,1,0],[1,0,0],[1,0,0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2,1,1],[3,1,2],[3,3,4]]\n",
    "y_test = [[0,0,1],[0,0,1],[0,0,1]]\n",
    "\n",
    "X = tf.placeholder(\"float\",[None,3])\n",
    "Y = tf.placeholder(\"float\",[None,3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3,3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis),axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-10).minimize(cost)\n",
    "\n",
    "# correct predicstion test model\n",
    "prediction = tf.arg_max(hypothesis,1)\n",
    "is_correct = tf.equal(prediction,tf.arg_max(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        if step % 20 == 0:\n",
    "            cost_val, W_val, _ = sess.run([cost,W,optimizer],feed_dict={X:x_data,Y:y_data})\n",
    "            print(\"step:\",step,\"cost:\",cost_val,\"W:\",W_val)\n",
    "        \n",
    "    #predict\n",
    "    print(\"Prediction:\",sess.run(prediction,feed_dict={X:x_test}))\n",
    "    \n",
    "    #Calculate accuracy\n",
    "    print(\"Accuracy:\",sess.run(accuracy,feed_dict={X:x_test,Y:y_test}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  6.81751e+09 \n",
      "Prediction:\n",
      " [[  59588.5078125]\n",
      " [ 117468.515625 ]\n",
      " [  92916.046875 ]\n",
      " [  65844.296875 ]\n",
      " [  77180.9765625]\n",
      " [  77806.5703125]\n",
      " [  71493.09375  ]\n",
      " [  90363.8359375]]\n",
      "1 Cost:  7.49014e+24 \n",
      "Prediction:\n",
      " [[ -1.93052921e+12]\n",
      " [ -3.88635322e+12]\n",
      " [ -3.05725388e+12]\n",
      " [ -2.14311882e+12]\n",
      " [ -2.52577998e+12]\n",
      " [ -2.54703881e+12]\n",
      " [ -2.33444921e+12]\n",
      " [ -2.97221803e+12]]\n",
      "2 Cost:  inf \n",
      "Prediction:\n",
      " [[  6.39899978e+19]\n",
      " [  1.28818422e+20]\n",
      " [  1.01336806e+20]\n",
      " [  7.10365612e+19]\n",
      " [  8.37203866e+19]\n",
      " [  8.44250416e+19]\n",
      " [  7.73784739e+19]\n",
      " [  9.85181770e+19]]\n",
      "3 Cost:  inf \n",
      "Prediction:\n",
      " [[ -2.12103467e+27]\n",
      " [ -4.26986018e+27]\n",
      " [ -3.35894490e+27]\n",
      " [ -2.35460262e+27]\n",
      " [ -2.77502488e+27]\n",
      " [ -2.79838170e+27]\n",
      " [ -2.56481375e+27]\n",
      " [ -3.26551790e+27]]\n",
      "4 Cost:  inf \n",
      "Prediction:\n",
      " [[  7.03045559e+34]\n",
      " [  1.41530288e+35]\n",
      " [  1.11336772e+35]\n",
      " [  7.80464844e+34]\n",
      " [  9.19819556e+34]\n",
      " [  9.27561435e+34]\n",
      " [  8.50142150e+34]\n",
      " [  1.08240000e+35]]\n",
      "5 Cost:  inf \n",
      "Prediction:\n",
      " [[-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]]\n",
      "6 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "7 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "8 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "9 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "10 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "11 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "12 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "13 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "14 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "15 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "16 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "17 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "18 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "19 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "20 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "21 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "22 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "23 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "24 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "25 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "26 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "27 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "28 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "29 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "30 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "31 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "32 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "33 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "34 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "35 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "36 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "37 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "38 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "39 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "40 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "41 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "42 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "43 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "44 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "45 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "46 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "47 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "48 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "49 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "50 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "51 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "52 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "53 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "54 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "55 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "56 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "57 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "58 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "59 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "60 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "61 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "62 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "63 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "64 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "65 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "66 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "67 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "68 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "69 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "70 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "71 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "72 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "73 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "74 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "75 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "76 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "77 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "78 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "79 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "80 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "81 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "82 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "83 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "84 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "85 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "86 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "87 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "88 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "89 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "90 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "91 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "92 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "93 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "94 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "95 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "96 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "97 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "98 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "99 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n",
      "100 Cost:  nan \n",
      "Prediction:\n",
      " [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n"
     ]
    }
   ],
   "source": [
    "# Third, this is example for non-normalized input\n",
    "# result shows nan in cost\n",
    "\n",
    "%reset -f\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  0.871757 \n",
      "Prediction:\n",
      " [[ 1.57320106]\n",
      " [ 2.06587982]\n",
      " [ 1.59932864]\n",
      " [ 1.09536862]\n",
      " [ 1.3530283 ]\n",
      " [ 1.39104867]\n",
      " [ 0.95598572]\n",
      " [ 1.25087345]]\n",
      "1 Cost:  0.871699 \n",
      "Prediction:\n",
      " [[ 1.57315874]\n",
      " [ 2.06583643]\n",
      " [ 1.59929252]\n",
      " [ 1.09534061]\n",
      " [ 1.3529954 ]\n",
      " [ 1.39101672]\n",
      " [ 0.95596308]\n",
      " [ 1.25085044]]\n",
      "2 Cost:  0.87164 \n",
      "Prediction:\n",
      " [[ 1.57311654]\n",
      " [ 2.06579304]\n",
      " [ 1.59925628]\n",
      " [ 1.0953126 ]\n",
      " [ 1.35296249]\n",
      " [ 1.39098477]\n",
      " [ 0.95594049]\n",
      " [ 1.25082743]]\n",
      "3 Cost:  0.871581 \n",
      "Prediction:\n",
      " [[ 1.57307434]\n",
      " [ 2.06574965]\n",
      " [ 1.59922028]\n",
      " [ 1.09528458]\n",
      " [ 1.35292959]\n",
      " [ 1.39095283]\n",
      " [ 0.95591784]\n",
      " [ 1.25080442]]\n",
      "4 Cost:  0.871522 \n",
      "Prediction:\n",
      " [[ 1.57303214]\n",
      " [ 2.06570649]\n",
      " [ 1.59918404]\n",
      " [ 1.09525657]\n",
      " [ 1.35289669]\n",
      " [ 1.39092088]\n",
      " [ 0.95589525]\n",
      " [ 1.25078142]]\n",
      "5 Cost:  0.871464 \n",
      "Prediction:\n",
      " [[ 1.57299018]\n",
      " [ 2.0656631 ]\n",
      " [ 1.59914804]\n",
      " [ 1.09522855]\n",
      " [ 1.35286379]\n",
      " [ 1.39088893]\n",
      " [ 0.95587265]\n",
      " [ 1.25075841]]\n",
      "6 Cost:  0.871405 \n",
      "Prediction:\n",
      " [[ 1.57294774]\n",
      " [ 2.06561995]\n",
      " [ 1.59911203]\n",
      " [ 1.09520054]\n",
      " [ 1.35283089]\n",
      " [ 1.39085698]\n",
      " [ 0.95585001]\n",
      " [ 1.2507354 ]]\n",
      "7 Cost:  0.871346 \n",
      "Prediction:\n",
      " [[ 1.57290578]\n",
      " [ 2.06557655]\n",
      " [ 1.59907579]\n",
      " [ 1.09517252]\n",
      " [ 1.35279799]\n",
      " [ 1.39082503]\n",
      " [ 0.95582747]\n",
      " [ 1.25071239]]\n",
      "8 Cost:  0.871287 \n",
      "Prediction:\n",
      " [[ 1.57286346]\n",
      " [ 2.0655334 ]\n",
      " [ 1.59903979]\n",
      " [ 1.09514451]\n",
      " [ 1.35276508]\n",
      " [ 1.39079309]\n",
      " [ 0.95580482]\n",
      " [ 1.25068951]]\n",
      "9 Cost:  0.871229 \n",
      "Prediction:\n",
      " [[ 1.57282138]\n",
      " [ 2.06549025]\n",
      " [ 1.59900379]\n",
      " [ 1.09511662]\n",
      " [ 1.35273218]\n",
      " [ 1.39076114]\n",
      " [ 0.95578229]\n",
      " [ 1.2506665 ]]\n",
      "10 Cost:  0.87117 \n",
      "Prediction:\n",
      " [[ 1.57277918]\n",
      " [ 2.06544685]\n",
      " [ 1.59896767]\n",
      " [ 1.09508848]\n",
      " [ 1.35269928]\n",
      " [ 1.39072919]\n",
      " [ 0.95575964]\n",
      " [ 1.25064349]]\n",
      "11 Cost:  0.871111 \n",
      "Prediction:\n",
      " [[ 1.57273698]\n",
      " [ 2.0654037 ]\n",
      " [ 1.59893155]\n",
      " [ 1.09506059]\n",
      " [ 1.35266638]\n",
      " [ 1.39069724]\n",
      " [ 0.95573705]\n",
      " [ 1.25062048]]\n",
      "12 Cost:  0.871053 \n",
      "Prediction:\n",
      " [[ 1.57269478]\n",
      " [ 2.06536031]\n",
      " [ 1.59889555]\n",
      " [ 1.09503245]\n",
      " [ 1.35263348]\n",
      " [ 1.39066529]\n",
      " [ 0.95571446]\n",
      " [ 1.25059748]]\n",
      "13 Cost:  0.870994 \n",
      "Prediction:\n",
      " [[ 1.57265282]\n",
      " [ 2.06531715]\n",
      " [ 1.59885931]\n",
      " [ 1.09500456]\n",
      " [ 1.35260057]\n",
      " [ 1.39063334]\n",
      " [ 0.95569187]\n",
      " [ 1.25057459]]\n",
      "14 Cost:  0.870935 \n",
      "Prediction:\n",
      " [[ 1.57261038]\n",
      " [ 2.065274  ]\n",
      " [ 1.59882331]\n",
      " [ 1.09497654]\n",
      " [ 1.35256767]\n",
      " [ 1.3906014 ]\n",
      " [ 0.95566928]\n",
      " [ 1.25055158]]\n",
      "15 Cost:  0.870876 \n",
      "Prediction:\n",
      " [[ 1.57256842]\n",
      " [ 2.06523061]\n",
      " [ 1.59878731]\n",
      " [ 1.09494853]\n",
      " [ 1.35253477]\n",
      " [ 1.39056957]\n",
      " [ 0.95564669]\n",
      " [ 1.25052857]]\n",
      "16 Cost:  0.870818 \n",
      "Prediction:\n",
      " [[ 1.5725261 ]\n",
      " [ 2.06518745]\n",
      " [ 1.59875107]\n",
      " [ 1.09492052]\n",
      " [ 1.35250187]\n",
      " [ 1.3905375 ]\n",
      " [ 0.9556241 ]\n",
      " [ 1.25050569]]\n",
      "17 Cost:  0.870759 \n",
      "Prediction:\n",
      " [[ 1.57248402]\n",
      " [ 2.0651443 ]\n",
      " [ 1.59871507]\n",
      " [ 1.0948925 ]\n",
      " [ 1.35246897]\n",
      " [ 1.39050567]\n",
      " [ 0.95560145]\n",
      " [ 1.25048256]]\n",
      "18 Cost:  0.8707 \n",
      "Prediction:\n",
      " [[ 1.57244182]\n",
      " [ 2.06510091]\n",
      " [ 1.59867907]\n",
      " [ 1.09486449]\n",
      " [ 1.35243607]\n",
      " [ 1.39047372]\n",
      " [ 0.95557892]\n",
      " [ 1.25045967]]\n",
      "19 Cost:  0.870642 \n",
      "Prediction:\n",
      " [[ 1.57239962]\n",
      " [ 2.06505775]\n",
      " [ 1.59864306]\n",
      " [ 1.09483647]\n",
      " [ 1.35240316]\n",
      " [ 1.39044189]\n",
      " [ 0.95555627]\n",
      " [ 1.25043666]]\n",
      "20 Cost:  0.870583 \n",
      "Prediction:\n",
      " [[ 1.57235742]\n",
      " [ 2.0650146 ]\n",
      " [ 1.59860682]\n",
      " [ 1.09480846]\n",
      " [ 1.35237026]\n",
      " [ 1.39040995]\n",
      " [ 0.95553368]\n",
      " [ 1.25041366]]\n",
      "21 Cost:  0.870524 \n",
      "Prediction:\n",
      " [[ 1.57231545]\n",
      " [ 2.06497121]\n",
      " [ 1.59857082]\n",
      " [ 1.09478045]\n",
      " [ 1.35233748]\n",
      " [ 1.390378  ]\n",
      " [ 0.95551109]\n",
      " [ 1.25039077]]\n",
      "22 Cost:  0.870466 \n",
      "Prediction:\n",
      " [[ 1.57227302]\n",
      " [ 2.06492805]\n",
      " [ 1.59853482]\n",
      " [ 1.09475255]\n",
      " [ 1.35230446]\n",
      " [ 1.39034605]\n",
      " [ 0.9554885 ]\n",
      " [ 1.25036764]]\n",
      "23 Cost:  0.870407 \n",
      "Prediction:\n",
      " [[ 1.57223105]\n",
      " [ 2.06488466]\n",
      " [ 1.5984987 ]\n",
      " [ 1.09472442]\n",
      " [ 1.35227156]\n",
      " [ 1.3903141 ]\n",
      " [ 0.95546591]\n",
      " [ 1.25034475]]\n",
      "24 Cost:  0.870348 \n",
      "Prediction:\n",
      " [[ 1.57218874]\n",
      " [ 2.06484151]\n",
      " [ 1.59846258]\n",
      " [ 1.0946964 ]\n",
      " [ 1.35223877]\n",
      " [ 1.39028215]\n",
      " [ 0.95544332]\n",
      " [ 1.25032175]]\n",
      "25 Cost:  0.87029 \n",
      "Prediction:\n",
      " [[ 1.57214665]\n",
      " [ 2.06479836]\n",
      " [ 1.59842658]\n",
      " [ 1.09466839]\n",
      " [ 1.35220587]\n",
      " [ 1.39025021]\n",
      " [ 0.95542073]\n",
      " [ 1.25029874]]\n",
      "26 Cost:  0.870231 \n",
      "Prediction:\n",
      " [[ 1.57210445]\n",
      " [ 2.06475496]\n",
      " [ 1.59839058]\n",
      " [ 1.09464049]\n",
      " [ 1.35217297]\n",
      " [ 1.39021826]\n",
      " [ 0.95539808]\n",
      " [ 1.25027585]]\n",
      "27 Cost:  0.870172 \n",
      "Prediction:\n",
      " [[ 1.57206225]\n",
      " [ 2.06471181]\n",
      " [ 1.59835434]\n",
      " [ 1.09461236]\n",
      " [ 1.35214019]\n",
      " [ 1.39018631]\n",
      " [ 0.95537549]\n",
      " [ 1.25025272]]\n",
      "28 Cost:  0.870114 \n",
      "Prediction:\n",
      " [[ 1.57202005]\n",
      " [ 2.06466866]\n",
      " [ 1.59831834]\n",
      " [ 1.09458447]\n",
      " [ 1.35210717]\n",
      " [ 1.39015436]\n",
      " [ 0.9553529 ]\n",
      " [ 1.25022984]]\n",
      "29 Cost:  0.870055 \n",
      "Prediction:\n",
      " [[ 1.57197809]\n",
      " [ 2.06462526]\n",
      " [ 1.59828234]\n",
      " [ 1.09455645]\n",
      " [ 1.35207427]\n",
      " [ 1.39012241]\n",
      " [ 0.95533031]\n",
      " [ 1.25020683]]\n",
      "30 Cost:  0.869997 \n",
      "Prediction:\n",
      " [[ 1.57193577]\n",
      " [ 2.06458211]\n",
      " [ 1.59824622]\n",
      " [ 1.09452844]\n",
      " [ 1.35204148]\n",
      " [ 1.39009058]\n",
      " [ 0.95530778]\n",
      " [ 1.25018394]]\n",
      "31 Cost:  0.869938 \n",
      "Prediction:\n",
      " [[ 1.57189369]\n",
      " [ 2.06453896]\n",
      " [ 1.5982101 ]\n",
      " [ 1.09450054]\n",
      " [ 1.35200858]\n",
      " [ 1.39005876]\n",
      " [ 0.95528525]\n",
      " [ 1.25016093]]\n",
      "32 Cost:  0.869879 \n",
      "Prediction:\n",
      " [[ 1.57185149]\n",
      " [ 2.0644958 ]\n",
      " [ 1.59817421]\n",
      " [ 1.09447265]\n",
      " [ 1.3519758 ]\n",
      " [ 1.39002681]\n",
      " [ 0.95526272]\n",
      " [ 1.25013804]]\n",
      "33 Cost:  0.869821 \n",
      "Prediction:\n",
      " [[ 1.57180953]\n",
      " [ 2.06445265]\n",
      " [ 1.59813809]\n",
      " [ 1.09444463]\n",
      " [ 1.35194302]\n",
      " [ 1.38999498]\n",
      " [ 0.95524013]\n",
      " [ 1.25011516]]\n",
      "34 Cost:  0.869762 \n",
      "Prediction:\n",
      " [[ 1.57176733]\n",
      " [ 2.06440949]\n",
      " [ 1.59810221]\n",
      " [ 1.09441662]\n",
      " [ 1.35191011]\n",
      " [ 1.38996315]\n",
      " [ 0.9552176 ]\n",
      " [ 1.25009227]]\n",
      "35 Cost:  0.869704 \n",
      "Prediction:\n",
      " [[ 1.57172537]\n",
      " [ 2.06436634]\n",
      " [ 1.59806609]\n",
      " [ 1.09438872]\n",
      " [ 1.35187733]\n",
      " [ 1.3899312 ]\n",
      " [ 0.95519507]\n",
      " [ 1.25006938]]\n",
      "36 Cost:  0.869645 \n",
      "Prediction:\n",
      " [[ 1.57168317]\n",
      " [ 2.06432295]\n",
      " [ 1.59803009]\n",
      " [ 1.09436083]\n",
      " [ 1.35184443]\n",
      " [ 1.38989937]\n",
      " [ 0.95517254]\n",
      " [ 1.25004637]]\n",
      "37 Cost:  0.869587 \n",
      "Prediction:\n",
      " [[ 1.57164121]\n",
      " [ 2.06427979]\n",
      " [ 1.59799409]\n",
      " [ 1.09433281]\n",
      " [ 1.35181165]\n",
      " [ 1.38986754]\n",
      " [ 0.95515001]\n",
      " [ 1.25002348]]\n",
      "38 Cost:  0.869528 \n",
      "Prediction:\n",
      " [[ 1.57159889]\n",
      " [ 2.06423664]\n",
      " [ 1.59795809]\n",
      " [ 1.09430492]\n",
      " [ 1.35177875]\n",
      " [ 1.3898356 ]\n",
      " [ 0.95512748]\n",
      " [ 1.25000048]]\n",
      "39 Cost:  0.86947 \n",
      "Prediction:\n",
      " [[ 1.57155681]\n",
      " [ 2.06419349]\n",
      " [ 1.59792209]\n",
      " [ 1.09427691]\n",
      " [ 1.35174596]\n",
      " [ 1.38980377]\n",
      " [ 0.95510495]\n",
      " [ 1.24997759]]\n",
      "40 Cost:  0.869411 \n",
      "Prediction:\n",
      " [[ 1.57151461]\n",
      " [ 2.06415033]\n",
      " [ 1.59788609]\n",
      " [ 1.09424901]\n",
      " [ 1.35171318]\n",
      " [ 1.38977194]\n",
      " [ 0.95508242]\n",
      " [ 1.2499547 ]]\n",
      "41 Cost:  0.869353 \n",
      "Prediction:\n",
      " [[ 1.57147264]\n",
      " [ 2.06410718]\n",
      " [ 1.59785008]\n",
      " [ 1.094221  ]\n",
      " [ 1.35168028]\n",
      " [ 1.38973999]\n",
      " [ 0.95505989]\n",
      " [ 1.24993181]]\n",
      "42 Cost:  0.869294 \n",
      "Prediction:\n",
      " [[ 1.57143044]\n",
      " [ 2.06406403]\n",
      " [ 1.59781408]\n",
      " [ 1.0941931 ]\n",
      " [ 1.35164738]\n",
      " [ 1.38970816]\n",
      " [ 0.95503736]\n",
      " [ 1.2499088 ]]\n",
      "43 Cost:  0.869236 \n",
      "Prediction:\n",
      " [[ 1.57138848]\n",
      " [ 2.06402063]\n",
      " [ 1.59777808]\n",
      " [ 1.09416521]\n",
      " [ 1.35161471]\n",
      " [ 1.38967633]\n",
      " [ 0.95501482]\n",
      " [ 1.24988592]]\n",
      "44 Cost:  0.869177 \n",
      "Prediction:\n",
      " [[ 1.57134628]\n",
      " [ 2.06397772]\n",
      " [ 1.59774208]\n",
      " [ 1.09413719]\n",
      " [ 1.35158181]\n",
      " [ 1.38964438]\n",
      " [ 0.95499229]\n",
      " [ 1.24986291]]\n",
      "45 Cost:  0.869119 \n",
      "Prediction:\n",
      " [[ 1.57130432]\n",
      " [ 2.06393433]\n",
      " [ 1.59770608]\n",
      " [ 1.0941093 ]\n",
      " [ 1.35154891]\n",
      " [ 1.38961256]\n",
      " [ 0.95496976]\n",
      " [ 1.24984002]]\n",
      "46 Cost:  0.86906 \n",
      "Prediction:\n",
      " [[ 1.571262  ]\n",
      " [ 2.06389117]\n",
      " [ 1.59766996]\n",
      " [ 1.0940814 ]\n",
      " [ 1.35151613]\n",
      " [ 1.38958073]\n",
      " [ 0.95494723]\n",
      " [ 1.24981713]]\n",
      "47 Cost:  0.869002 \n",
      "Prediction:\n",
      " [[ 1.57121992]\n",
      " [ 2.06384802]\n",
      " [ 1.59763408]\n",
      " [ 1.09405339]\n",
      " [ 1.35148335]\n",
      " [ 1.38954878]\n",
      " [ 0.95492464]\n",
      " [ 1.24979424]]\n",
      "48 Cost:  0.868943 \n",
      "Prediction:\n",
      " [[ 1.57117772]\n",
      " [ 2.06380486]\n",
      " [ 1.59759796]\n",
      " [ 1.09402537]\n",
      " [ 1.35145044]\n",
      " [ 1.38951695]\n",
      " [ 0.95490211]\n",
      " [ 1.24977136]]\n",
      "49 Cost:  0.868885 \n",
      "Prediction:\n",
      " [[ 1.57113576]\n",
      " [ 2.06376171]\n",
      " [ 1.59756196]\n",
      " [ 1.09399748]\n",
      " [ 1.35141766]\n",
      " [ 1.38948512]\n",
      " [ 0.95487958]\n",
      " [ 1.24974835]]\n",
      "50 Cost:  0.868826 \n",
      "Prediction:\n",
      " [[ 1.57109356]\n",
      " [ 2.06371856]\n",
      " [ 1.59752595]\n",
      " [ 1.09396958]\n",
      " [ 1.35138488]\n",
      " [ 1.38945317]\n",
      " [ 0.95485699]\n",
      " [ 1.24972546]]\n",
      "51 Cost:  0.868767 \n",
      "Prediction:\n",
      " [[ 1.5710516 ]\n",
      " [ 2.06367517]\n",
      " [ 1.59748983]\n",
      " [ 1.09394157]\n",
      " [ 1.35135198]\n",
      " [ 1.38942122]\n",
      " [ 0.95483452]\n",
      " [ 1.24970257]]\n",
      "52 Cost:  0.868709 \n",
      "Prediction:\n",
      " [[ 1.5710094 ]\n",
      " [ 2.06363201]\n",
      " [ 1.59745395]\n",
      " [ 1.09391356]\n",
      " [ 1.35131907]\n",
      " [ 1.3893894 ]\n",
      " [ 0.95481193]\n",
      " [ 1.24967957]]\n",
      "53 Cost:  0.868651 \n",
      "Prediction:\n",
      " [[ 1.57096744]\n",
      " [ 2.0635891 ]\n",
      " [ 1.59741783]\n",
      " [ 1.09388566]\n",
      " [ 1.35128629]\n",
      " [ 1.38935757]\n",
      " [ 0.9547894 ]\n",
      " [ 1.24965668]]\n",
      "54 Cost:  0.868592 \n",
      "Prediction:\n",
      " [[ 1.57092512]\n",
      " [ 2.0635457 ]\n",
      " [ 1.59738183]\n",
      " [ 1.09385777]\n",
      " [ 1.35125351]\n",
      " [ 1.38932574]\n",
      " [ 0.95476687]\n",
      " [ 1.24963379]]\n",
      "55 Cost:  0.868534 \n",
      "Prediction:\n",
      " [[ 1.57088304]\n",
      " [ 2.06350255]\n",
      " [ 1.59734583]\n",
      " [ 1.09382975]\n",
      " [ 1.35122061]\n",
      " [ 1.38929391]\n",
      " [ 0.95474434]\n",
      " [ 1.2496109 ]]\n",
      "56 Cost:  0.868475 \n",
      "Prediction:\n",
      " [[ 1.57084084]\n",
      " [ 2.0634594 ]\n",
      " [ 1.59730983]\n",
      " [ 1.09380186]\n",
      " [ 1.35118771]\n",
      " [ 1.38926196]\n",
      " [ 0.95472181]\n",
      " [ 1.24958789]]\n",
      "57 Cost:  0.868417 \n",
      "Prediction:\n",
      " [[ 1.57079887]\n",
      " [ 2.06341624]\n",
      " [ 1.59727383]\n",
      " [ 1.09377384]\n",
      " [ 1.35115504]\n",
      " [ 1.38923001]\n",
      " [ 0.95469928]\n",
      " [ 1.24956501]]\n",
      "58 Cost:  0.868358 \n",
      "Prediction:\n",
      " [[ 1.57075667]\n",
      " [ 2.06337309]\n",
      " [ 1.59723783]\n",
      " [ 1.09374595]\n",
      " [ 1.35112214]\n",
      " [ 1.38919818]\n",
      " [ 0.95467675]\n",
      " [ 1.249542  ]]\n",
      "59 Cost:  0.8683 \n",
      "Prediction:\n",
      " [[ 1.57071471]\n",
      " [ 2.06332994]\n",
      " [ 1.59720182]\n",
      " [ 1.09371805]\n",
      " [ 1.35108924]\n",
      " [ 1.38916636]\n",
      " [ 0.95465422]\n",
      " [ 1.24951911]]\n",
      "60 Cost:  0.868241 \n",
      "Prediction:\n",
      " [[ 1.57067251]\n",
      " [ 2.06328678]\n",
      " [ 1.59716582]\n",
      " [ 1.09369004]\n",
      " [ 1.35105646]\n",
      " [ 1.38913441]\n",
      " [ 0.95463169]\n",
      " [ 1.24949622]]\n",
      "61 Cost:  0.868183 \n",
      "Prediction:\n",
      " [[ 1.57063055]\n",
      " [ 2.06324339]\n",
      " [ 1.59712982]\n",
      " [ 1.09366214]\n",
      " [ 1.35102367]\n",
      " [ 1.38910258]\n",
      " [ 0.95460916]\n",
      " [ 1.24947333]]\n",
      "62 Cost:  0.868124 \n",
      "Prediction:\n",
      " [[ 1.57058835]\n",
      " [ 2.06320047]\n",
      " [ 1.59709382]\n",
      " [ 1.09363413]\n",
      " [ 1.35099077]\n",
      " [ 1.38907075]\n",
      " [ 0.95458663]\n",
      " [ 1.24945045]]\n",
      "63 Cost:  0.868066 \n",
      "Prediction:\n",
      " [[ 1.57054639]\n",
      " [ 2.06315732]\n",
      " [ 1.59705782]\n",
      " [ 1.09360623]\n",
      " [ 1.35095811]\n",
      " [ 1.3890388 ]\n",
      " [ 0.95456409]\n",
      " [ 1.24942744]]\n",
      "64 Cost:  0.868007 \n",
      "Prediction:\n",
      " [[ 1.57050419]\n",
      " [ 2.06311417]\n",
      " [ 1.59702182]\n",
      " [ 1.09357834]\n",
      " [ 1.35092521]\n",
      " [ 1.38900709]\n",
      " [ 0.95454156]\n",
      " [ 1.24940455]]\n",
      "65 Cost:  0.867949 \n",
      "Prediction:\n",
      " [[ 1.57046223]\n",
      " [ 2.06307101]\n",
      " [ 1.59698582]\n",
      " [ 1.09355044]\n",
      " [ 1.35089242]\n",
      " [ 1.38897514]\n",
      " [ 0.95451903]\n",
      " [ 1.24938154]]\n",
      "66 Cost:  0.867891 \n",
      "Prediction:\n",
      " [[ 1.57042003]\n",
      " [ 2.06302786]\n",
      " [ 1.59694982]\n",
      " [ 1.09352243]\n",
      " [ 1.35085964]\n",
      " [ 1.38894331]\n",
      " [ 0.9544965 ]\n",
      " [ 1.24935865]]\n",
      "67 Cost:  0.867832 \n",
      "Prediction:\n",
      " [[ 1.57037807]\n",
      " [ 2.0629847 ]\n",
      " [ 1.59691381]\n",
      " [ 1.09349453]\n",
      " [ 1.35082674]\n",
      " [ 1.38891149]\n",
      " [ 0.95447397]\n",
      " [ 1.24933577]]\n",
      "68 Cost:  0.867774 \n",
      "Prediction:\n",
      " [[ 1.57033587]\n",
      " [ 2.06294155]\n",
      " [ 1.59687781]\n",
      " [ 1.09346652]\n",
      " [ 1.35079396]\n",
      " [ 1.38887954]\n",
      " [ 0.95445144]\n",
      " [ 1.24931288]]\n",
      "69 Cost:  0.867715 \n",
      "Prediction:\n",
      " [[ 1.5702939 ]\n",
      " [ 2.0628984 ]\n",
      " [ 1.59684181]\n",
      " [ 1.09343863]\n",
      " [ 1.35076118]\n",
      " [ 1.38884783]\n",
      " [ 0.95442891]\n",
      " [ 1.24928987]]\n",
      "70 Cost:  0.867657 \n",
      "Prediction:\n",
      " [[ 1.5702517 ]\n",
      " [ 2.06285524]\n",
      " [ 1.59680581]\n",
      " [ 1.09341073]\n",
      " [ 1.35072827]\n",
      " [ 1.38881588]\n",
      " [ 0.95440638]\n",
      " [ 1.24926698]]\n",
      "71 Cost:  0.867598 \n",
      "Prediction:\n",
      " [[ 1.57020974]\n",
      " [ 2.06281185]\n",
      " [ 1.59676981]\n",
      " [ 1.09338284]\n",
      " [ 1.35069549]\n",
      " [ 1.38878405]\n",
      " [ 0.95438385]\n",
      " [ 1.24924409]]\n",
      "72 Cost:  0.86754 \n",
      "Prediction:\n",
      " [[ 1.57016754]\n",
      " [ 2.06276894]\n",
      " [ 1.59673381]\n",
      " [ 1.09335494]\n",
      " [ 1.35066271]\n",
      " [ 1.38875222]\n",
      " [ 0.95436132]\n",
      " [ 1.24922109]]\n",
      "73 Cost:  0.867481 \n",
      "Prediction:\n",
      " [[ 1.57012558]\n",
      " [ 2.06272554]\n",
      " [ 1.59669781]\n",
      " [ 1.09332693]\n",
      " [ 1.35062981]\n",
      " [ 1.38872027]\n",
      " [ 0.95433879]\n",
      " [ 1.2491982 ]]\n",
      "74 Cost:  0.867423 \n",
      "Prediction:\n",
      " [[ 1.57008338]\n",
      " [ 2.06268239]\n",
      " [ 1.59666181]\n",
      " [ 1.09329903]\n",
      " [ 1.35059702]\n",
      " [ 1.38868845]\n",
      " [ 0.95431626]\n",
      " [ 1.24917531]]\n",
      "75 Cost:  0.867365 \n",
      "Prediction:\n",
      " [[ 1.57004142]\n",
      " [ 2.06263924]\n",
      " [ 1.5966258 ]\n",
      " [ 1.09327102]\n",
      " [ 1.35056424]\n",
      " [ 1.38865662]\n",
      " [ 0.95429373]\n",
      " [ 1.24915242]]\n",
      "76 Cost:  0.867306 \n",
      "Prediction:\n",
      " [[ 1.56999922]\n",
      " [ 2.06259608]\n",
      " [ 1.5965898 ]\n",
      " [ 1.09324312]\n",
      " [ 1.35053146]\n",
      " [ 1.38862467]\n",
      " [ 0.9542712 ]\n",
      " [ 1.24912941]]\n",
      "77 Cost:  0.867248 \n",
      "Prediction:\n",
      " [[ 1.56995726]\n",
      " [ 2.06255293]\n",
      " [ 1.5965538 ]\n",
      " [ 1.09321523]\n",
      " [ 1.35049868]\n",
      " [ 1.38859296]\n",
      " [ 0.95424867]\n",
      " [ 1.24910653]]\n",
      "78 Cost:  0.867189 \n",
      "Prediction:\n",
      " [[ 1.56991506]\n",
      " [ 2.06250978]\n",
      " [ 1.5965178 ]\n",
      " [ 1.09318733]\n",
      " [ 1.35046577]\n",
      " [ 1.38856101]\n",
      " [ 0.95422614]\n",
      " [ 1.24908364]]\n",
      "79 Cost:  0.867131 \n",
      "Prediction:\n",
      " [[ 1.56987309]\n",
      " [ 2.06246662]\n",
      " [ 1.5964818 ]\n",
      " [ 1.09315932]\n",
      " [ 1.35043299]\n",
      " [ 1.38852918]\n",
      " [ 0.95420361]\n",
      " [ 1.24906063]]\n",
      "80 Cost:  0.867073 \n",
      "Prediction:\n",
      " [[ 1.56983089]\n",
      " [ 2.06242347]\n",
      " [ 1.5964458 ]\n",
      " [ 1.0931313 ]\n",
      " [ 1.35040021]\n",
      " [ 1.38849735]\n",
      " [ 0.95418108]\n",
      " [ 1.24903774]]\n",
      "81 Cost:  0.867014 \n",
      "Prediction:\n",
      " [[ 1.56978893]\n",
      " [ 2.06238031]\n",
      " [ 1.5964098 ]\n",
      " [ 1.09310341]\n",
      " [ 1.35036731]\n",
      " [ 1.3884654 ]\n",
      " [ 0.95415854]\n",
      " [ 1.24901485]]\n",
      "82 Cost:  0.866956 \n",
      "Prediction:\n",
      " [[ 1.56974673]\n",
      " [ 2.06233716]\n",
      " [ 1.5963738 ]\n",
      " [ 1.09307551]\n",
      " [ 1.35033453]\n",
      " [ 1.38843369]\n",
      " [ 0.95413601]\n",
      " [ 1.24899197]]\n",
      "83 Cost:  0.866897 \n",
      "Prediction:\n",
      " [[ 1.56970477]\n",
      " [ 2.06229401]\n",
      " [ 1.5963378 ]\n",
      " [ 1.09304762]\n",
      " [ 1.35030174]\n",
      " [ 1.38840175]\n",
      " [ 0.95411348]\n",
      " [ 1.24896896]]\n",
      "84 Cost:  0.866839 \n",
      "Prediction:\n",
      " [[ 1.56966257]\n",
      " [ 2.06225085]\n",
      " [ 1.59630179]\n",
      " [ 1.09301972]\n",
      " [ 1.35026884]\n",
      " [ 1.3883698 ]\n",
      " [ 0.95409095]\n",
      " [ 1.24894607]]\n",
      "85 Cost:  0.866781 \n",
      "Prediction:\n",
      " [[ 1.56962061]\n",
      " [ 2.0622077 ]\n",
      " [ 1.59626579]\n",
      " [ 1.09299171]\n",
      " [ 1.35023606]\n",
      " [ 1.38833809]\n",
      " [ 0.95406842]\n",
      " [ 1.24892306]]\n",
      "86 Cost:  0.866722 \n",
      "Prediction:\n",
      " [[ 1.56957841]\n",
      " [ 2.06216455]\n",
      " [ 1.59622979]\n",
      " [ 1.09296381]\n",
      " [ 1.35020328]\n",
      " [ 1.38830614]\n",
      " [ 0.95404589]\n",
      " [ 1.24890018]]\n",
      "87 Cost:  0.866664 \n",
      "Prediction:\n",
      " [[ 1.56953645]\n",
      " [ 2.06212139]\n",
      " [ 1.59619391]\n",
      " [ 1.0929358 ]\n",
      " [ 1.35017049]\n",
      " [ 1.38827431]\n",
      " [ 0.95402336]\n",
      " [ 1.24887729]]\n",
      "88 Cost:  0.866605 \n",
      "Prediction:\n",
      " [[ 1.56949425]\n",
      " [ 2.06207824]\n",
      " [ 1.59615779]\n",
      " [ 1.09290791]\n",
      " [ 1.35013759]\n",
      " [ 1.38824248]\n",
      " [ 0.95400083]\n",
      " [ 1.2488544 ]]\n",
      "89 Cost:  0.866547 \n",
      "Prediction:\n",
      " [[ 1.56945229]\n",
      " [ 2.06203508]\n",
      " [ 1.59612179]\n",
      " [ 1.09288001]\n",
      " [ 1.35010481]\n",
      " [ 1.38821054]\n",
      " [ 0.9539783 ]\n",
      " [ 1.24883151]]\n",
      "90 Cost:  0.866489 \n",
      "Prediction:\n",
      " [[ 1.56941009]\n",
      " [ 2.06199193]\n",
      " [ 1.59608591]\n",
      " [ 1.09285212]\n",
      " [ 1.35007203]\n",
      " [ 1.38817871]\n",
      " [ 0.95395577]\n",
      " [ 1.2488085 ]]\n",
      "91 Cost:  0.86643 \n",
      "Prediction:\n",
      " [[ 1.56936812]\n",
      " [ 2.06194878]\n",
      " [ 1.59604979]\n",
      " [ 1.0928241 ]\n",
      " [ 1.35003924]\n",
      " [ 1.38814688]\n",
      " [ 0.95393324]\n",
      " [ 1.24878561]]\n",
      "92 Cost:  0.866372 \n",
      "Prediction:\n",
      " [[ 1.56932592]\n",
      " [ 2.06190538]\n",
      " [ 1.5960139 ]\n",
      " [ 1.09279621]\n",
      " [ 1.35000634]\n",
      " [ 1.38811505]\n",
      " [ 0.95391071]\n",
      " [ 1.24876261]]\n",
      "93 Cost:  0.866314 \n",
      "Prediction:\n",
      " [[ 1.56928396]\n",
      " [ 2.06186247]\n",
      " [ 1.5959779 ]\n",
      " [ 1.09276819]\n",
      " [ 1.34997356]\n",
      " [ 1.38808322]\n",
      " [ 0.95388818]\n",
      " [ 1.24873972]]\n",
      "94 Cost:  0.866255 \n",
      "Prediction:\n",
      " [[ 1.56924176]\n",
      " [ 2.06181908]\n",
      " [ 1.5959419 ]\n",
      " [ 1.0927403 ]\n",
      " [ 1.34994078]\n",
      " [ 1.38805127]\n",
      " [ 0.95386565]\n",
      " [ 1.24871683]]\n",
      "95 Cost:  0.866197 \n",
      "Prediction:\n",
      " [[ 1.5691998 ]\n",
      " [ 2.06177592]\n",
      " [ 1.5959059 ]\n",
      " [ 1.0927124 ]\n",
      " [ 1.34990788]\n",
      " [ 1.38801932]\n",
      " [ 0.95384312]\n",
      " [ 1.24869394]]\n",
      "96 Cost:  0.866138 \n",
      "Prediction:\n",
      " [[ 1.5691576 ]\n",
      " [ 2.06173277]\n",
      " [ 1.5958699 ]\n",
      " [ 1.09268451]\n",
      " [ 1.34987509]\n",
      " [ 1.38798761]\n",
      " [ 0.95382059]\n",
      " [ 1.24867105]]\n",
      "97 Cost:  0.86608 \n",
      "Prediction:\n",
      " [[ 1.56911564]\n",
      " [ 2.06168985]\n",
      " [ 1.5958339 ]\n",
      " [ 1.09265649]\n",
      " [ 1.34984231]\n",
      " [ 1.38795567]\n",
      " [ 0.95379806]\n",
      " [ 1.24864805]]\n",
      "98 Cost:  0.866022 \n",
      "Prediction:\n",
      " [[ 1.56907344]\n",
      " [ 2.06164646]\n",
      " [ 1.5957979 ]\n",
      " [ 1.0926286 ]\n",
      " [ 1.34980953]\n",
      " [ 1.38792384]\n",
      " [ 0.95377553]\n",
      " [ 1.24862516]]\n",
      "99 Cost:  0.865963 \n",
      "Prediction:\n",
      " [[ 1.56903148]\n",
      " [ 2.06160331]\n",
      " [ 1.59576178]\n",
      " [ 1.0926007 ]\n",
      " [ 1.34977674]\n",
      " [ 1.38789201]\n",
      " [ 0.95375299]\n",
      " [ 1.24860215]]\n",
      "100 Cost:  0.865905 \n",
      "Prediction:\n",
      " [[ 1.56898928]\n",
      " [ 2.06156015]\n",
      " [ 1.59572601]\n",
      " [ 1.09257269]\n",
      " [ 1.34974384]\n",
      " [ 1.38786018]\n",
      " [ 0.95373046]\n",
      " [ 1.24857926]]\n"
     ]
    }
   ],
   "source": [
    "# To fix above problem, using normalized input\n",
    "\n",
    "%reset -f\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define min-max to make noramlized data set\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "# Apply normalization. Without this, this example wouldn't work\n",
    "xy = MinMaxScaler(xy)\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "# This is MNIST data set example\n",
    "# MNIST data set is used to recognize human hand-written number into machine. Used a lot in USPS system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data # Call MNIST data from TF example lib\n",
    "\n",
    "# For plotting purpose\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost: 2.5032706\n",
      "Epoch: 0002 cost: 1.06783852\n",
      "Epoch: 0003 cost: 0.857558025\n",
      "Epoch: 0004 cost: 0.753253832\n",
      "Epoch: 0005 cost: 0.687812025\n",
      "Epoch: 0006 cost: 0.640982842\n",
      "Epoch: 0007 cost: 0.605896605\n",
      "Epoch: 0008 cost: 0.577272361\n",
      "Epoch: 0009 cost: 0.553793345\n",
      "Epoch: 0010 cost: 0.533783167\n",
      "Epoch: 0011 cost: 0.516848599\n",
      "Epoch: 0012 cost: 0.502127431\n",
      "Epoch: 0013 cost: 0.4889979\n",
      "Epoch: 0014 cost: 0.47720372\n",
      "Epoch: 0015 cost: 0.466277443\n",
      "Learning done!\n",
      "Accuracy: 0.8904\n",
      "label: [1]\n",
      "prediction: [1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsvxSUZZfKsryS0VZfqkoyy8V9f9TMmS/DPNdxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x31c17c080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "#MNIST data image of shape 28*28 = 784\n",
    "X = tf.placeholder(tf.float32,[None,784])\n",
    "#0-9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32,[None,nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784,nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "training_epochs = 15 # one epoch : one forward pass and one backward of all the training examples\n",
    "batch_size = 100 # Define size of training set in whole training set\n",
    "# Ex : 1000 training examples, if batch size is 500, then it will take 2 iterations to complete 1 epoch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) # ex : 10000/100, defines how many batch cycles\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost,optimizer],feed_dict={X:batch_xs,Y:batch_ys})\n",
    "            avg_cost += c/total_batch\n",
    "        print('Epoch:','%04d' % (epoch+1), 'cost:','{:.9}'.format(avg_cost))\n",
    "        \n",
    "    print(\"Learning done!\")\n",
    "    \n",
    "    # Test the model using test sets\n",
    "    print('Accuracy:',accuracy.eval(session=sess,feed_dict={X:mnist.test.images,Y:mnist.test.labels}))\n",
    "    # above is same as sess.run(accuracy,feed_dict={X:mnist.test.images,Y:mnist.test.labels})\n",
    "    \n",
    "    # Get one and predict\n",
    "    r = random.randint(0,mnist.test.num_examples - 1) # Read random number\n",
    "    print('label:',sess.run(tf.argmax(mnist.test.labels[r:r+1],1))) # That prints the label of number what we read\n",
    "    # Then using hypothesis to predict\n",
    "    print('prediction:',sess.run(tf.argmax(hypothesis,1),feed_dict={X:mnist.test.images[r:r+1]}))\n",
    "    \n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28,28),cmap='Greys',interpolation='nearest')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
