{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Flow basic : Lab 10\n",
    "### NN contiue\n",
    "Here, we keep discuss about NN. Especially, we observe how to overcome several problems in NN. (Following Hinton's argument)\n",
    "#### Better than sigmoid for backpropagation\n",
    "Previously, we see the idea of backpropagation (chain rule) to enable NN. This is good for few layers but many layers (for exmaple 9 hidden layers) case is not working well. This is because we are using `sigmoid` function for our hypothesis. Since `sigmod` is always less than 1, moving each hidden layer by backpropagation makes really small value (think in terms of chain rule for partial derivative) eventually input layer doesn't effect a lot to output layer. This is called **vanishing gradient**\n",
    "\n",
    "To resolve this issue, we use **Rectified Linear Unit (ReLU)**. ReLU is simple to represent $\\textrm{ReLu}(x) = \\max(0,x)$. For NN, we are using ReLu rather than sigmoid i.e. define hypothesis in TF\n",
    "```\n",
    "L1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "-> L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "```\n",
    "ReLU has different options\n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{Leak-ReLU}(x) = \\max(0.1 x , x) \\\\\n",
    "\\textrm{ELU}(x) =\n",
    "\\begin{cases}\n",
    "x & \\textrm{if}\\,\\,\\,\\, x>0\\\\\n",
    "\\alpha(e^x-1) & \\textrm{if} \\,\\,\\,\\, x\\leq0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Also, there are different activation functions (used as hypothesis in layers) such as $\\tanh(x)$ and $\\textrm{maxout}(x) = \\max(w_1^T x + b_1,w_2^T x + b_2)$\n",
    "\n",
    "#### Good initializtion for weight\n",
    "Chaging activation function provides better performance. However, we also need to consider initial weight to get better performance. For example, if we set initial weight as zero then all forwarding gradient will be zero i.e. can fail to use NN.\n",
    "\n",
    "To set the initial weight values wisely:\n",
    "* Not all 0s\n",
    "* Hinton et al. (2006), \"A fast learning algorithm for Deep Belief Nets (DBNs)\n",
    "   - Restricted Boatman Machine (RBM)\n",
    "\n",
    "And there are other ways to do this. \n",
    "\n",
    "We can use RBM to initialize weights\n",
    "* Apply the RBM idae on adjacent two layers as a pre-training step\n",
    "* Continue the first process to all layers\n",
    "* This will set weights\n",
    "\n",
    "This routine is an example of DBN and this is pre-training stage. (Don't need to worry about $x$ values). \n",
    "\n",
    "Recently, several different initializations which are less complicated than RBM such as **Xavier initialization** and **He's initialization**. The basic ideas are that makes sure the weights are just right, not too small, not too big then using number of input (fan_in) and output (fan_out). In the python language:\n",
    "```\n",
    "# Xavier initialization\n",
    "# Glorot et al. 2010\n",
    "W = np.random.number(fan_in, fan_out)/np.sqrt(fan_in)\n",
    "\n",
    "# He et al. 2015\n",
    "W = np.random.number(fan_in, fan_out)/np.sqrt(fan_in/2)\n",
    "\n",
    "```\n",
    "This is still an active area of research to find the way to initialize perfect weight values. Many new algorithms such as batch normalization, layer sequential uniform variance are coming out\n",
    "\n",
    "#### Dropout and model ensemble\n",
    "We nees to avoid overfitting issue. Overfitting causes very high accuracy on the training dataset but poor accuracy on the test data set. Further, errors are incresing with more layers for overfitting case. Previously, we knew that regularization such that\n",
    "\n",
    "\\begin{align}\n",
    "L = \\textrm{cost} + \\lambda \\sum W^2\n",
    "\\end{align}\n",
    "\n",
    "where $\\lambda$ is regularization strength. In `tf`, `l2req = 0.001*tf.reduce_sum(tf.square(W))`\n",
    "\n",
    "In NN, we have **Dropout** : a simple way to prevent NNs from overfitting->Randomly set some neurons to zero in the forward pass. In TF:\n",
    "```\n",
    "dropout_rate = tf.placeholer(\"float\")\n",
    "_L1 = tf.nn.relu(tf.add(tf.matmul(X,W1),B1))\n",
    "L1 = tf.nn.dropout(_L1,dropout_rate)\n",
    "\n",
    "#Train\n",
    "sess.run(optimizer, feed_dict={X:batch_xs,Y:batch_ys,dropout_rate:0.7})\n",
    "```\n",
    "\n",
    "Dropout is used for training only. For actual evaluation, we need to set `dropout_rate=1`\n",
    "\n",
    "Ensemble is the idea combining with multiple training sets and learning models to make ensemble prediction. It increases 2~5% performance\n",
    "\n",
    "#### Different ways to build networks\n",
    "So far, we only use feedforward NN (like build lego blocks sequentially). There are differnt ways to build network modules such as Fast forward (less net structure from He's paper), split & merge, recurrent network (RNN) etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Here we recall softmax classifier for MNIST (can be found in tf_lab7)\n",
    "# Previously, we only use one softmax classifier (again go to tf_lab7) and it still gives good accuracy (~90%)\n",
    "# now we build NN for MNIST\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# Call MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 179.951563021\n",
      "Epoch: 0002 cost = 41.559866098\n",
      "Epoch: 0003 cost = 26.066673390\n",
      "Epoch: 0004 cost = 18.315359251\n",
      "Epoch: 0005 cost = 13.380362025\n",
      "Epoch: 0006 cost = 10.052926301\n",
      "Epoch: 0007 cost = 7.593942371\n",
      "Epoch: 0008 cost = 5.642096143\n",
      "Epoch: 0009 cost = 4.236297221\n",
      "Epoch: 0010 cost = 3.229735587\n",
      "Epoch: 0011 cost = 2.425976305\n",
      "Epoch: 0012 cost = 1.821693864\n",
      "Epoch: 0013 cost = 1.372106152\n",
      "Epoch: 0014 cost = 1.084839214\n",
      "Epoch: 0015 cost = 0.856817239\n",
      "Learning Finished!\n",
      "Accuracy: 0.9447\n",
      "Label:  [1]\n",
      "Prediction:  [1]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3240a6b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers (more than one)\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1) # We use ReLU instead of softmax\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "# Also use adam optimizer insted of GD (https://arxiv.org/pdf/1412.6980v8.pdf)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) \n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r+1].reshape(28,28),cmap='Greys',interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows better accuracy than one softmax function for hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, we use Xavier initialization (one way to get better weight)\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.reset_default_graph() # Remove all graphs\n",
    "\n",
    "mnist = input_data.read_data_sets(\"../data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.341126442\n",
      "Epoch: 0002 cost = 0.117148896\n",
      "Epoch: 0003 cost = 0.075436816\n",
      "Epoch: 0004 cost = 0.053076496\n",
      "Epoch: 0005 cost = 0.039920689\n",
      "Epoch: 0006 cost = 0.032030637\n",
      "Epoch: 0007 cost = 0.022917410\n",
      "Epoch: 0008 cost = 0.017558097\n",
      "Epoch: 0009 cost = 0.017216508\n",
      "Epoch: 0010 cost = 0.014961640\n",
      "Epoch: 0011 cost = 0.012652319\n",
      "Epoch: 0012 cost = 0.010476419\n",
      "Epoch: 0013 cost = 0.009686003\n",
      "Epoch: 0014 cost = 0.010394863\n",
      "Epoch: 0015 cost = 0.009171074\n",
      "Learning Finished!\n",
      "Accuracy: 0.9799\n",
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIxPCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3213c0400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers (more than one)\n",
    "# Every other parts code is same as above only add initializer for Ws\n",
    "W1 = tf.get_variable(\"W1\",shape=[784,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1) # We use ReLU instead of softmax\n",
    "\n",
    "W2 = tf.get_variable(\"W2\",shape=[256,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\",shape=[256,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r+1].reshape(28,28),cmap='Greys',interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now accuracy is 97.99% and initial cost value is also very small compare with previous result. This implies our initial setup for weight is good\n",
    "\n",
    "We can make deeper and wider NN. However, this cannot guarantee to get better accuracy. Deeper network might make overfitting case. Below example shows this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.reset_default_graph() # Remove all graphs\n",
    "mnist = input_data.read_data_sets(\"../data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.336836551\n",
      "Epoch: 0002 cost = 0.108269960\n",
      "Epoch: 0003 cost = 0.072398160\n",
      "Epoch: 0004 cost = 0.054412273\n",
      "Epoch: 0005 cost = 0.040992234\n",
      "Epoch: 0006 cost = 0.035164901\n",
      "Epoch: 0007 cost = 0.029615844\n",
      "Epoch: 0008 cost = 0.028432992\n",
      "Epoch: 0009 cost = 0.021891236\n",
      "Epoch: 0010 cost = 0.023325515\n",
      "Epoch: 0011 cost = 0.019095659\n",
      "Epoch: 0012 cost = 0.017563614\n",
      "Epoch: 0013 cost = 0.018490362\n",
      "Epoch: 0014 cost = 0.018734867\n",
      "Epoch: 0015 cost = 0.012367959\n",
      "Learning Finished!\n",
      "Accuracy: 0.9793\n",
      "Label:  [7]\n",
      "Prediction:  [7]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8ANBEX4gKMIPBEX4gaD+B2QiwJn2DhKDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x32a3f6320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers (more than one)\n",
    "# make deeper and wider\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r+1].reshape(28,28),cmap='Greys',interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "which result is not improved. So we are using dropout to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.reset_default_graph() # Remove all graphs\n",
    "mnist = input_data.read_data_sets(\"../data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.484515907\n",
      "Epoch: 0002 cost = 0.173634511\n",
      "Epoch: 0003 cost = 0.131325629\n",
      "Epoch: 0004 cost = 0.109560412\n",
      "Epoch: 0005 cost = 0.093561091\n",
      "Epoch: 0006 cost = 0.085864961\n",
      "Epoch: 0007 cost = 0.075527771\n",
      "Epoch: 0008 cost = 0.068908536\n",
      "Epoch: 0009 cost = 0.064786779\n",
      "Epoch: 0010 cost = 0.058208433\n",
      "Epoch: 0011 cost = 0.057146338\n",
      "Epoch: 0012 cost = 0.052961456\n",
      "Epoch: 0013 cost = 0.052528372\n",
      "Epoch: 0014 cost = 0.048297470\n",
      "Epoch: 0015 cost = 0.045722039\n",
      "Learning Finished!\n",
      "Accuracy: 0.9804\n",
      "Label:  [7]\n",
      "Prediction:  [7]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCA3aG8nvuxsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x320483e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Dropout rate\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers (more than one)\n",
    "# Apply dropout\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob:0.7}# For training, keep_prob value is set around 0.5~0.7\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob:1})) # To test, we need to use all nets so set keep_prob:1\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob:1}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r+1].reshape(28,28),cmap='Greys',interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "we reach 98% accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
